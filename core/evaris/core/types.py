"""Core type definitions for Evaris evaluation framework.

This module defines the fundamental data structures used throughout Evaris:
- Golden: Ground truth test data
- TestCase: Complete test case with actual output
- MetricResult: Single metric evaluation result
- TestResult: Complete test case result
- EvalResult: Aggregated evaluation results
- MissingRequirementError: Exception for missing metric requirements
"""

from collections.abc import Awaitable, Callable
from pathlib import Path
from typing import TYPE_CHECKING, Any, Literal

from pydantic import BaseModel, ConfigDict, Field


class MissingRequirementError(Exception):
    """Raised when a metric is missing required inputs.

    This exception signals that a metric should be skipped rather than
    failing with an error. The evaluation framework catches this exception
    and marks the metric as "skipped" instead of "error".

    Attributes:
        metric_name: Name of the metric that's missing requirements
        missing_keys: List of missing required metadata keys
        available_keys: List of keys that are available in metadata

    Example:
        >>> raise MissingRequirementError(
        ...     metric_name="faithfulness",
        ...     missing_keys=["context"],
        ...     available_keys=["difficulty"]
        ... )
    """

    def __init__(
        self,
        metric_name: str,
        missing_keys: list[str],
        available_keys: list[str] | None = None,
    ):
        self.metric_name = metric_name
        self.missing_keys = missing_keys
        self.available_keys = available_keys or []
        missing_str = ", ".join(f"'{k}'" for k in missing_keys)
        available_str = (
            ", ".join(f"'{k}'" for k in self.available_keys) if self.available_keys else "none"
        )
        super().__init__(
            f"{metric_name} requires {missing_str} in test_case.metadata. "
            f"Available keys: [{available_str}]"
        )


if TYPE_CHECKING:
    from evaris.baselines import BaselineComparisonReport


# Multi-modal input/output types for agent interface
# Supports text, images, audio, video, structured data
MultiModalInput = str | dict[str, Any] | list[str | Path] | Path
MultiModalOutput = str | dict[str, Any] | list[Any]


class Golden(BaseModel):
    """Static test data without actual outputs (ground truth).

    This can usually be data through which the model is finetuned,
    tested or evaluated against.

    Goldens provide a strong base to compare/evaluate against actual
    output.

    Attributes:
        input: The input to be sent to the agent
        expected: The expected/golden output (optional)
        metadata: Additional metadata for the golden

    Example:
        >>> golden = Golden(
        ...     input="What is the capital of France?",
        ...     expected="Paris",
        ...     metadata={"category": "geography"}
        ... )
    """

    model_config = ConfigDict(arbitrary_types_allowed=True)

    input: Any = Field(..., description="Input to the agent")
    expected: Any | None = Field(None, description="Expected/golden output")
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional metadata for the golden"
    )


class TestCase(BaseModel):
    """A complete test case with actual output for evaluation.

    TestCase extends Golden by adding the actual_output field, which
    stores the agent's response. This is used for evaluation.

    Attributes:
        input: The input to be sent to the agent
        expected: The expected/golden output (optional)
        actual_output: The agent's actual response (optional, for offline eval)
        metadata: Additional metadata for the test case

    Example:
        >>> test_case = TestCase(
        ...     input="What is 2+2?",
        ...     expected="4",
        ...     actual_output="4",
        ...     metadata={"difficulty": "easy"}
        ... )
    """

    model_config = ConfigDict(arbitrary_types_allowed=True)

    input: Any = Field(..., description="Input to the agent")
    expected: Any | None = Field(None, description="Expected output from the agent")
    actual_output: Any | None = Field(
        None, description="Pre-computed actual output (for offline evaluation)"
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional metadata for the test case"
    )

    @classmethod
    def from_golden(cls, golden: Golden, actual_output: Any = None) -> "TestCase":
        """Create a TestCase from a Golden and an actual output.

        Args:
            golden: The golden test data (input + expected)
            actual_output: The actual output generated by the agent (optional)

        Returns:
            TestCase: A complete test case ready for evaluation

        Note:
            Metadata is copied from Golden to prevent accidental mutation
            of the original Golden's metadata.
        """
        return cls(
            input=golden.input,
            expected=golden.expected,
            actual_output=actual_output,
            metadata=golden.metadata.copy() if golden.metadata else {},
        )


class ReasoningStep(BaseModel):
    """A single step in the reasoning trace for metric evaluation.

    This model captures intermediate values and logic during metric evaluation,
    providing transparency into how scores are calculated. Essential for
    ABC compliance (explainability requirement).

    Attributes:
        step_number: Sequential step number (1-indexed)
        operation: Type of operation performed
        description: Human-readable description
        inputs: Input values for this step
        outputs: Output values from this step
        metadata: Additional step-specific metadata

    Example:
        >>> step = ReasoningStep(
        ...     step_number=1,
        ...     operation="text_normalization",
        ...     description="Normalizing text before comparison",
        ...     inputs={"text": "Paris"},
        ...     outputs={"normalized": "paris"}
        ... )
    """

    step_number: int = Field(..., description="Sequential step number (1-indexed)")
    operation: str = Field(..., description="Type of operation performed")
    description: str = Field(..., description="Human-readable description of this step")
    inputs: dict[str, Any] = Field(..., description="Input values for this step")
    outputs: dict[str, Any] = Field(..., description="Output values from this step")
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional step-specific metadata"
    )


MetricStatus = Literal["passed", "failed", "skipped", "error"]


class MetricResult(BaseModel):
    """Result from a single metric evaluation.

    Contains the score, pass/fail status, and optional reasoning for
    transparency into how the metric calculated the score.

    Attributes:
        name: Metric name
        score: Score between 0 and 1
        passed: Whether the test passed
        status: Evaluation status (passed, failed, skipped, error)
        metadata: Additional metric-specific data
        reasoning: Human-readable reasoning summary
        reasoning_steps: Structured trace of reasoning steps
        reasoning_type: Type of reasoning ('logic', 'llm', 'hybrid')

    Status values:
        - passed: Metric ran successfully and score >= threshold
        - failed: Metric ran successfully but score < threshold
        - skipped: Metric was skipped (missing required inputs)
        - error: Metric encountered an error during evaluation

    Example:
        >>> result = MetricResult(
        ...     name="exact_match",
        ...     score=1.0,
        ...     passed=True,
        ...     reasoning="Exact match after normalization"
        ... )
    """

    name: str = Field(..., description="Metric name")
    score: float = Field(..., ge=0.0, le=1.0, description="Score between 0 and 1")
    passed: bool = Field(..., description="Whether the test passed")
    status: MetricStatus | None = Field(
        default=None,
        description=(
            "Evaluation status: 'passed' (score >= threshold), 'failed' (score < threshold), "
            "'skipped' (missing requirements), 'error' (evaluation error)"
        ),
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional metric-specific data"
    )

    # Reasoning fields (optional, for transparency)
    reasoning: str | None = Field(
        default=None, description="Human-readable reasoning summary explaining the score"
    )
    reasoning_steps: list[ReasoningStep] | None = Field(
        default=None, description="Structured trace of reasoning steps taken during evaluation"
    )
    reasoning_type: Literal["logic", "llm", "hybrid"] | None = Field(
        default=None,
        description=(
            "Type of reasoning used: 'logic' for rule-based, "
            "'llm' for LLM-based, 'hybrid' for both"
        ),
    )

    # Cost tracking fields (for LLM-based metrics)
    input_tokens: int | None = Field(
        default=None, description="Number of input/prompt tokens used by LLM"
    )
    output_tokens: int | None = Field(
        default=None, description="Number of output/completion tokens used by LLM"
    )
    total_tokens: int | None = Field(default=None, description="Total tokens used (input + output)")
    cost_usd: float | None = Field(
        default=None, description="Cost in USD for this metric evaluation"
    )


class TestResult(BaseModel):
    """Result from evaluating a single test case.

    Contains the test case, actual output, all metric results,
    execution latency, and any errors that occurred.

    Attributes:
        test_case: The test case that was evaluated
        output: Actual output from the agent
        metrics: List of metric results
        latency_ms: Execution time in milliseconds
        error: Error message if execution failed
    """

    test_case: TestCase = Field(..., description="The test case that was evaluated")
    output: Any = Field(..., description="Actual output from the agent")
    metrics: list[MetricResult] = Field(..., description="Metric results")
    latency_ms: float = Field(..., ge=0.0, description="Execution time in milliseconds")
    error: str | None = Field(None, description="Error message if execution failed")

    # Cost tracking (aggregated from all metrics)
    total_tokens: int | None = Field(
        default=None, description="Total tokens used across all metrics"
    )
    cost_usd: float | None = Field(default=None, description="Total cost in USD for this test case")


class EvalResult(BaseModel):
    """Aggregated results from an evaluation run.

    Contains summary statistics, all individual test results,
    and optional baseline comparison data.

    Attributes:
        name: Evaluation name
        total: Total number of test cases
        passed: Number of passed test cases
        failed: Number of failed test cases
        accuracy: Overall accuracy (passed/total)
        avg_latency_ms: Average latency in milliseconds
        results: Individual test results
        metadata: Additional evaluation metadata
        baseline_results: Results for each baseline
        baseline_comparison: Comparison report
    """

    name: str = Field(..., description="Evaluation name")
    total: int = Field(..., ge=0, description="Total number of test cases")
    passed: int = Field(..., ge=0, description="Number of passed test cases")
    failed: int = Field(..., ge=0, description="Number of failed test cases")
    accuracy: float = Field(..., ge=0.0, le=1.0, description="Overall accuracy")
    avg_latency_ms: float = Field(..., ge=0.0, description="Average latency in milliseconds")
    results: list[TestResult] = Field(..., description="Individual test results")
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional evaluation metadata"
    )

    # Cost tracking (aggregated from all test results)
    total_tokens: int | None = Field(
        default=None, description="Total tokens used across entire evaluation"
    )
    total_cost_usd: float | None = Field(
        default=None, description="Total cost in USD for entire evaluation"
    )

    # Baseline comparison fields
    baseline_results: dict[str, "EvalResult"] | None = Field(
        None, description="Evaluation results for each baseline (baseline_name -> EvalResult)"
    )
    baseline_comparison: "BaselineComparisonReport | None" = Field(
        None, description="Comparison report between agent and baselines"
    )

    def __str__(self) -> str:
        """Human-readable summary of evaluation results."""
        return (
            f"Evaluation: {self.name}\n"
            f"Total: {self.total} | Passed: {self.passed} | Failed: {self.failed}\n"
            f"Accuracy: {self.accuracy:.2%} | Avg Latency: {self.avg_latency_ms:.2f}ms"
        )


# Type aliases for better readability
AgentFunction = Callable[[Any], Any]
AsyncAgentFunction = Callable[[Any], Awaitable[Any]]
DatasetInput = list[dict[str, Any]] | list[TestCase] | list[Golden]
